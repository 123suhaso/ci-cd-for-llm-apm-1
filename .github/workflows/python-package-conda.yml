name: CI/CD AKS Deployment with Automatic Rollback

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: read
  packages: write

on:
  push:
    branches:
      - main

env:
  IMAGE_TAG: ${{ github.sha }}
  KEYVAULT_NAME: kv-scopebot
  DEPLOYMENT_TIMEOUT: 600s
  HEALTH_CHECK_RETRIES: 10
  SMOKE_TEST_RETRIES: 5
  SERVICE_WAIT_TIMEOUT: 300
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository_owner }}/llm-apm

jobs:
  code-quality-frontend:
    name: Frontend — Syntax & Build Check
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      - name: Install dependencies
        run: |
          if [ -f package-lock.json ]; then
            npm ci
          else
            npm install
          fi
      - name: Rebuild native modules
        run: npm rebuild || true
      - name: Format (Prettier)
        run: npm run format --if-present || true
      - name: Build syntax check
        run: npm run build --if-present

  code-quality-backend:
    name: Backend — Python Syntax Check
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: 3.11
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt
      - name: Install dependencies (non-fatal)
        run: |
          python -m pip install --upgrade pip setuptools wheel
          python -m pip install -r requirements.txt || true
      - name: Python syntax check
        run: python -m compileall . || true

  security-checks:
    name: Security Checks (Bandit, Safety, ESLint)
    runs-on: ubuntu-latest
    needs: [code-quality-frontend, code-quality-backend]
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          fetch-tags: true
          persist-credentials: true

      - uses: actions/setup-python@v4
        with:
          python-version: 3.11
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt
      - name: Install Python deps + tools
        run: |
          cd backend || exit 0
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt || true
          python -m pip install bandit safety || true
          cd ..
      - name: Run Bandit
        run: |
          if [ -d backend ]; then
            cd backend
            bandit -r . -f json -o ../bandit-report.json || true
            cd ..
          fi
      - name: Run Safety
        run: |
          if [ -f backend/requirements.txt ]; then
            safety check -r backend/requirements.txt --json > safety-report.json || true
          fi
      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      - name: Run ESLint
        run: |
          if [ -d frontend ]; then
            cd frontend
            npm install || true
            npx eslint . --ext .js,.jsx || true
            cd ..
          fi
      - uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  backend-tests:
    name: Backend — Run pytest (with coverage)
    runs-on: ubuntu-latest
    needs: [code-quality-backend]
    defaults:
      run:
        working-directory: backend
    env:
      DATABASE_URL: "postgresql://ci:ci@127.0.0.1:5432/ci_db"
      PYTHONPATH: "${{ github.workspace }}/backend"
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: 3.11
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies (including pytest-cov)
        run: |
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install pytest pytest-asyncio httpx pytest-cov || true

      - name: Run pytest + coverage
        id: pytest
        run: |
          pytest -q --maxfail=1 \
            --junitxml=backend-tests.xml \
            --cov=. \
            --cov-report=term \
            --cov-report=xml:coverage.xml \
            --cov-fail-under=0 || pytest_exit=$?
          if [ -n "${pytest_exit:-}" ]; then
            echo "pytest failed with code ${pytest_exit}"
            exit ${pytest_exit}
          fi

      - name: Upload pytest JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-pytest-report
          path: backend/backend-tests.xml

      - name: Upload coverage xml
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backend-coverage
          path: backend/coverage.xml

  build:
    name: Build & Push Docker Images
    runs-on: ubuntu-latest
    needs: [security-checks, backend-tests]
    strategy:
      matrix:
        service: [backend, frontend]
    steps:
      - uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PAT }}

      - name: Pre-pull previous cache images (best-effort)
        run: |
          echo "Attempting to pull buildcache and latest for ${SERVICE:=${{ matrix.service }}}"
          docker pull ${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:buildcache || true
          docker pull ${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:latest || true

      - name: Build & Push Docker image (with registry cache)
        uses: docker/build-push-action@v5
        with:
          context: ${{ matrix.service }}
          file: ${{ matrix.service }}/Dockerfile
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:${{ env.IMAGE_TAG }}
            ${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:latest
          cache-from: |
            type=registry,ref=${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:buildcache
            type=registry,ref=${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:latest
          cache-to: |
            type=registry,ref=${{ secrets.DOCKER_USERNAME }}/llm-apm-${{ matrix.service }}:buildcache,mode=max
            type=local,dest=/tmp/.buildx-cache-new,mode=max

      - name: Save buildx cache (optional)
        run: |
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true

  container-scan:
    name: Container Image Security Scan (Trivy)
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        service: [backend, frontend]
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sudo sh -s -- -b /usr/local/bin
          trivy --version

      - name: Scan Docker image with Trivy
        run: |
          IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}-${{ matrix.service }}:${{ env.IMAGE_TAG }}"
          trivy image \
            --format table \
            --severity HIGH,CRITICAL \
            --ignore-unfixed \
            --timeout 5m \
            "${IMAGE}" || true


  deploy:
    name: Deploy to AKS with Auto-Rollback
    runs-on: ubuntu-latest
    needs: container-scan
    environment:
      name: production-environment
      url: https://your-app-url.com

    steps:
      - uses: actions/checkout@v4
      
      - uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - id: get-azure-info
        run: |
          TENANT_ID=$(az account show --query tenantId -o tsv)
          SUBSCRIPTION_ID=$(az account show --query id -o tsv)
          echo "tenantId=$TENANT_ID" >> $GITHUB_OUTPUT
          echo "subscriptionId=$SUBSCRIPTION_ID" >> $GITHUB_OUTPUT
      
      - uses: azure/setup-kubectl@v3
      - uses: azure/setup-helm@v3
      
      - name: Get AKS Credentials
        run: |
          az aks get-credentials \
            --resource-group ${{ secrets.AKS_RG }} \
            --name ${{ secrets.AKS_CLUSTER }} \
            --overwrite-existing

      - name: Ensure metrics-server is installed
        run: |
          if ! kubectl get deployment metrics-server -n kube-system >/dev/null 2>&1; then
            echo "Installing metrics-server..."
            kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
            kubectl rollout status deployment/metrics-server -n kube-system --timeout=120s || true
          else
            echo "metrics-server already present"
          fi

      - name: Ensure Managed Identity has Key Vault access
        run: |
          MI_OBJECT_ID=$(az aks show \
            --resource-group ${{ secrets.AKS_RG }} \
            --name ${{ secrets.AKS_CLUSTER }} \
            --query "identityProfile.kubeletidentity.objectId" -o tsv)
          az role assignment create \
            --role "Key Vault Secrets User" \
            --assignee-object-id "$MI_OBJECT_ID" \
            --assignee-principal-type ServicePrincipal \
            --scope "/subscriptions/${{ steps.get-azure-info.outputs.subscriptionId }}/resourceGroups/${{ secrets.AKS_RG }}/providers/Microsoft.KeyVault/vaults/${{ env.KEYVAULT_NAME }}" \
            || echo "Role already exists"

      - name: Create namespaces
        run: |
          kubectl create namespace llm-apm --dry-run=client -o yaml | kubectl apply -f -
          kubectl create namespace ingress-nginx --dry-run=client -o yaml | kubectl apply -f -

      - name: Create GHCR pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ secrets.ADMIN_EMAIL }} \
            -n llm-apm --dry-run=client -o yaml | kubectl apply -f -

      - name: Patch default service account
        run: |
          kubectl -n llm-apm patch serviceaccount default \
            --type='merge' -p '{"imagePullSecrets":[{"name":"ghcr-secret"}]}' || true

      # ============================================================
      # CAPTURE CURRENT STATE BEFORE DEPLOYMENT
      # ============================================================
      - name: Capture current deployment state
        id: capture-state
        run: |
          echo "📸 Capturing current deployment state..."
          
          if helm list -n llm-apm | grep -q llm-apm; then
            CURRENT_REVISION=$(helm list -n llm-apm -o json | jq -r '.[] | select(.name=="llm-apm") | .revision')
            echo "current_revision=$CURRENT_REVISION" >> $GITHUB_OUTPUT
            echo "deployment_exists=true" >> $GITHUB_OUTPUT
            echo "✅ Found existing deployment at revision $CURRENT_REVISION"
            
            helm get values llm-apm -n llm-apm > current-values.yaml || true
            kubectl get pods -n llm-apm -o wide > current-pods.txt || true
            
            echo "📊 Current Helm History:"
            helm history llm-apm -n llm-apm
          else
            echo "deployment_exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  No existing deployment found. This is a fresh install."
          fi

      # ============================================================
      # DEPLOY WITH AUTOMATIC ROLLBACK ON FAILURE
      # ============================================================
      - name: Deploy Helm chart with automatic rollback
        id: helm-deploy
        run: |
          echo "Starting deployment of version ${{ env.IMAGE_TAG }}..."
          
          set +e
          
          helm upgrade --install llm-apm ./llm-apm-chart \
            --namespace llm-apm \
            --create-namespace \
            --set imageRegistry=${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }} \
            --set imageTag=${{ env.IMAGE_TAG }} \
            --set backend.vpa.enabled=false \
            --set secretsProvider.tenantId=${{ steps.get-azure-info.outputs.tenantId }} \
            --atomic \
            --wait \
            --timeout=10m \
            --cleanup-on-fail \
            2>&1 | tee helm-deploy.log
          
          HELM_EXIT_CODE=${PIPESTATUS[0]}
          
          if [ $HELM_EXIT_CODE -ne 0 ]; then
            echo "❌ Helm deployment failed with exit code $HELM_EXIT_CODE"
            echo "deployment_status=failed" >> $GITHUB_OUTPUT
            cat helm-deploy.log
            exit 1
          else
            echo "✅ Helm deployment succeeded"
            echo "deployment_status=success" >> $GITHUB_OUTPUT
          fi

      # ============================================================
      # VERIFY SERVICES EXIST AFTER DEPLOYMENT
      # ============================================================
      - name: Verify services are created
        id: verify-services
        if: steps.helm-deploy.outputs.deployment_status == 'success'
        run: |
          echo "🔍 Verifying that services are created..."
          
          SERVICE_CHECK_FAILED=false
          MAX_WAIT=${{ env.SERVICE_WAIT_TIMEOUT }}
          INTERVAL=10
          
          REQUIRED_SERVICES=("backend-service" "frontend-service" "redis-service")
          
          for service in "${REQUIRED_SERVICES[@]}"; do
            echo "Checking for service: $service"
            elapsed=0
            while [ $elapsed -lt $MAX_WAIT ]; do
              if kubectl get svc "$service" -n llm-apm >/dev/null 2>&1; then
                echo "✅ Service $service exists"
                EP=$(kubectl get endpoints "$service" -n llm-apm -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || echo "")
                if [ -n "$EP" ]; then
                  echo "✅ Service $service has endpoints: $EP"
                  break
                else
                  echo "⏳ Service $service exists but has no endpoints yet ($elapsed/${MAX_WAIT}s)"
                fi
              else
                echo "⏳ Service $service not found yet ($elapsed/${MAX_WAIT}s)"
              fi
              sleep $INTERVAL
              elapsed=$((elapsed + INTERVAL))
            done

            if ! kubectl get svc "$service" -n llm-apm >/dev/null 2>&1; then
              echo "❌ Service $service was not created after ${MAX_WAIT}s"
              SERVICE_CHECK_FAILED=true
            fi
          done
          
          if [ "$SERVICE_CHECK_FAILED" = true ]; then
            echo "service_status=failed" >> $GITHUB_OUTPUT
            kubectl get svc -n llm-apm -o wide || true
            kubectl get pods -n llm-apm -o wide || true
            exit 1
          else
            echo "✅ All required services are created"
            echo "service_status=success" >> $GITHUB_OUTPUT
          fi

      # ============================================================
      # WAIT FOR ROLLOUTS TO COMPLETE
      # ============================================================
      - name: Wait for rollouts to complete
        id: wait-rollouts
        if: steps.verify-services.outputs.service_status == 'success'
        run: |
          echo "⏳ Waiting for rollouts to complete..."
          
          set +e
          
          kubectl rollout status deployment/backend -n llm-apm --timeout=${{ env.DEPLOYMENT_TIMEOUT }}
          BACKEND_STATUS=$?
          
          kubectl rollout status deployment/frontend -n llm-apm --timeout=${{ env.DEPLOYMENT_TIMEOUT }}
          FRONTEND_STATUS=$?
          
          kubectl rollout status deployment/redis -n llm-apm --timeout=300s
          REDIS_STATUS=$?
          
          if [ $BACKEND_STATUS -ne 0 ] || [ $FRONTEND_STATUS -ne 0 ] || [ $REDIS_STATUS -ne 0 ]; then
            echo "❌ Rollout failed!"
            echo "rollout_status=failed" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "✅ All rollouts completed successfully"
            echo "rollout_status=success" >> $GITHUB_OUTPUT
          fi

      # ============================================================
      # COMPREHENSIVE HEALTH CHECKS
      # ============================================================
      - name: Perform health checks
        id: health-checks
        if: steps.wait-rollouts.outputs.rollout_status == 'success'
        run: |
          echo "🏥 Performing health checks..."
          
          HEALTH_CHECK_FAILED=false
          
          kubectl get pods -n llm-apm
          
          if ! kubectl wait --for=condition=ready pod -l app=backend -n llm-apm --timeout=300s; then
            echo "❌ Backend pods not ready"
            HEALTH_CHECK_FAILED=true
          fi
          
          if ! kubectl wait --for=condition=ready pod -l app=frontend -n llm-apm --timeout=300s; then
            echo "❌ Frontend pods not ready"
            HEALTH_CHECK_FAILED=true
          fi
          
          UNHEALTHY_PODS=$(kubectl get pods -n llm-apm --field-selector=status.phase!=Running,status.phase!=Succeeded -o json | jq -r '.items | length')
          
          if [ "$UNHEALTHY_PODS" -gt 0 ]; then
            echo "❌ Found $UNHEALTHY_PODS unhealthy pods"
            kubectl get pods -n llm-apm --field-selector=status.phase!=Running,status.phase!=Succeeded
            HEALTH_CHECK_FAILED=true
          fi
          
          RESTART_COUNT=$(kubectl get pods -n llm-apm -o json | jq '[.items[].status.containerStatuses[]?.restartCount // 0] | add')
          echo "Total container restarts: $RESTART_COUNT"
          
          if [ "$RESTART_COUNT" -gt 5 ]; then
            echo "⚠️  High restart count: $RESTART_COUNT"
            HEALTH_CHECK_FAILED=true
          fi
          
          if [ "$HEALTH_CHECK_FAILED" = true ]; then
            echo "health_status=failed" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "✅ All health checks passed"
            echo "health_status=success" >> $GITHUB_OUTPUT
          fi

      # ============================================================
      # AUTOMATIC ROLLBACK ON ANY FAILURE
      # ============================================================
      - name: Automatic Rollback on Failure
        if: |
          failure() && 
          steps.capture-state.outputs.deployment_exists == 'true' &&
          (steps.helm-deploy.outcome == 'failure' || 
           steps.verify-services.outcome == 'failure' ||
           steps.wait-rollouts.outcome == 'failure' || 
           steps.health-checks.outcome == 'failure' || 
           steps.smoke-tests.outcome == 'failure')
        run: |
          echo "🔄 AUTOMATIC ROLLBACK INITIATED"
          echo "================================"
          
          if [ "${{ steps.helm-deploy.outcome }}" = "failure" ]; then
            echo "Reason: Helm deployment failed"
          elif [ "${{ steps.verify-services.outcome }}" = "failure" ]; then
            echo "Reason: Service verification failed"
          elif [ "${{ steps.wait-rollouts.outcome }}" = "failure" ]; then
            echo "Reason: Rollout timeout"
          elif [ "${{ steps.health-checks.outcome }}" = "failure" ]; then
            echo "Reason: Health checks failed"
          elif [ "${{ steps.smoke-tests.outcome }}" = "failure" ]; then
            echo "Reason: Smoke tests failed"
          fi
          
          echo ""
          echo "Rolling back to revision ${{ steps.capture-state.outputs.current_revision }}..."
          
          helm rollback llm-apm ${{ steps.capture-state.outputs.current_revision }} -n llm-apm --wait --timeout=5m
          
          echo "⏳ Waiting for rollback..."
          kubectl rollout status deployment/backend -n llm-apm --timeout=300s || true
          kubectl rollout status deployment/frontend -n llm-apm --timeout=300s || true
          kubectl rollout status deployment/redis -n llm-apm --timeout=300s || true
          
          sleep 15
          
          echo "✅ Rollback completed"
          kubectl get pods -n llm-apm -o wide
          helm history llm-apm -n llm-apm
          
          echo "❌ DEPLOYMENT FAILED - Rolled back to previous version"
          exit 1

      # ============================================================
      # INGRESS CONTROLLER SETUP
      # ============================================================
      # - name: Deploy NGINX Ingress Controller
      #   if: success()
      #   run: |
      #     helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
      #     helm repo update
      #     helm upgrade --install nginx-ingress ingress-nginx/ingress-nginx \
      #       --namespace ingress-nginx \
      #       --create-namespace \
      #       --set controller.service.type=LoadBalancer \
      #       --wait --timeout=5m

      - name: Get NGINX Ingress External IP
        if: success()
        run: |
          echo "Waiting for NGINX Ingress external IP..."
          for i in {1..60}; do
            IP=$(kubectl get svc -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o jsonpath='{.items[0].status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            if [ -n "$IP" ]; then
              echo "✅ NGINX Ingress IP: $IP"
              echo "NGINX_INGRESS_IP=$IP" >> $GITHUB_ENV
              break
            fi
            sleep 10
          done

      # ============================================================
      # SAVE DEPLOYMENT ARTIFACTS
      # ============================================================
      - name: Save deployment metadata
        if: always()
        run: |
          mkdir -p deployment-artifacts
          
          helm history llm-apm -n llm-apm > deployment-artifacts/helm-history.txt 2>&1 || true
          
          cat > deployment-artifacts/deployment-info.txt <<EOF
          IMAGE_TAG=${{ env.IMAGE_TAG }}
          REGISTRY=${{ env.REGISTRY }}
          GIT_SHA=${{ github.sha }}
          GIT_REF=${{ github.ref }}
          DEPLOYMENT_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          DEPLOYMENT_STATUS=${{ job.status }}
          PREVIOUS_REVISION=${{ steps.capture-state.outputs.current_revision }}
          EOF
          
          kubectl get pods -n llm-apm -o wide > deployment-artifacts/pod-status.txt 2>&1 || true
          kubectl get events -n llm-apm --sort-by='.lastTimestamp' > deployment-artifacts/events.txt 2>&1 || true
          helm get values llm-apm -n llm-apm > deployment-artifacts/deployed-values.yaml 2>&1 || true

      - name: Upload deployment artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deployment-artifacts-${{ github.sha }}
          path: deployment-artifacts/
          retention-days: 30

      # ============================================================
      # DEBUG INFORMATION ON FAILURE
      # ============================================================
      - name: Debug information on failure
        if: failure()
        run: |
          echo "======================================"
          echo "🔍 DEBUGGING INFORMATION"
          echo "======================================"
          
          echo "=== Pod Status ==="
          kubectl get pods -n llm-apm -o wide || true
          
          echo "=== Recent Events ==="
          kubectl get events -n llm-apm --sort-by='.lastTimestamp' | tail -50 || true
          
          echo "=== Backend Logs ==="
          kubectl logs -n llm-apm -l app=backend --tail=200 --all-containers=true || true
          
          echo "=== Frontend Logs ==="
          kubectl logs -n llm-apm -l app=frontend --tail=200 --all-containers=true || true
          
          echo "=== Failed Pod Details ==="
          for pod in $(kubectl get pods -n llm-apm --field-selector=status.phase!=Running -o name 2>/dev/null); do
            kubectl describe -n llm-apm $pod || true
          done